{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to reduce the file size. We have a total of 2.8M lines that we do not want to get rid of. Instead, we are going to focus on columns. In fact, there are 196 columns and we want to keep only a few that are going to be pertinent for our use.\n",
    "\n",
    "Runtime : approx. 1min 30s depending on RAM frequency (done on 128GB @ 3600MHz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the full file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to import the full data (ie. all the columns) and select only the columns we are interested in. The file being 7GB, we are going to import only a downsample of 1000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/en.openfoodfacts.org.products.csv\"\n",
    "df = pd.read_csv(filename, sep=\"\\t\", nrows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Columns: 196 entries, code to carnitine_100g\n",
      "dtypes: float64(127), int64(2), object(66), uint64(1)\n",
      "memory usage: 1.5+ MB\n",
      "None\n",
      "Index(['code', 'url', 'creator', 'created_t', 'created_datetime',\n",
      "       'last_modified_t', 'last_modified_datetime', 'product_name',\n",
      "       'abbreviated_product_name', 'generic_name',\n",
      "       ...\n",
      "       'carbon-footprint-from-meat-or-fish_100g', 'nutrition-score-fr_100g',\n",
      "       'nutrition-score-uk_100g', 'glycemic-index_100g', 'water-hardness_100g',\n",
      "       'choline_100g', 'phylloquinone_100g', 'beta-glucan_100g',\n",
      "       'inositol_100g', 'carnitine_100g'],\n",
      "      dtype='object', length=196)\n"
     ]
    }
   ],
   "source": [
    "print(df.info())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting the important columns only\n",
    "This list is personnaly created after exploring the dataset by hand (I used Dataiku so its not visible on this notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After exploration, we can select the columns we want to keep\n",
    "interesting_columns = ['url', 'product_name', 'generic_name', 'quantity', 'brands', 'categories', 'countries', 'ingredients_text', 'ingredients_tags', 'nutriscore_score', 'nutriscore_grade', 'pnns_groups_1', 'pnns_groups_2', 'food_groups', 'food_groups_tags', 'food_groups_en', 'completeness', 'main_category_en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deleting the variable\n",
    "This is done to reallocate RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the initial dataset into RAM with only the subset of interesting columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remS\\AppData\\Local\\Temp\\ipykernel_30516\\2170159026.py:1: DtypeWarning: Columns (9,54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(filename, usecols=interesting_columns, sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(filename, usecols=interesting_columns, sep='\\t')\n",
    "# We could add parameter \"low_memory=True\" to process the file in chunks, but it's not necessary here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking that the imported dataset if correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2614147 entries, 0 to 2614146\n",
      "Data columns (total 18 columns):\n",
      " #   Column            Dtype  \n",
      "---  ------            -----  \n",
      " 0   url               object \n",
      " 1   product_name      object \n",
      " 2   generic_name      object \n",
      " 3   quantity          object \n",
      " 4   brands            object \n",
      " 5   categories        object \n",
      " 6   countries         object \n",
      " 7   ingredients_text  object \n",
      " 8   ingredients_tags  object \n",
      " 9   nutriscore_score  float64\n",
      " 10  nutriscore_grade  object \n",
      " 11  pnns_groups_1     object \n",
      " 12  pnns_groups_2     object \n",
      " 13  food_groups       object \n",
      " 14  food_groups_tags  object \n",
      " 15  food_groups_en    object \n",
      " 16  completeness      float64\n",
      " 17  main_category_en  object \n",
      "dtypes: float64(2), object(16)\n",
      "memory usage: 359.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:  Index(['url', 'product_name', 'generic_name', 'quantity', 'brands',\n",
      "       'categories', 'countries', 'ingredients_text', 'ingredients_tags',\n",
      "       'nutriscore_score', 'nutriscore_grade', 'pnns_groups_1',\n",
      "       'pnns_groups_2', 'food_groups', 'food_groups_tags', 'food_groups_en',\n",
      "       'completeness', 'main_category_en'],\n",
      "      dtype='object')\n",
      "Shape:  (2614147, 18)\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns: \", data.columns)\n",
    "print(\"Shape: \", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting the new dataset that we will use in our processing notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data/reduced_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d67b4991c06ff004627933ee9d9d851ea096c7974ada14bae7f558f7e858c998"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
